<h1> About Cassandra Replication Factor and Consistency Level </h1>
<p style="text-align: right;"><em>Edge for Private Cloud v. 4.17.09</em></p>
<h2>About the Cassandra replication factor</h2>
<p>Cassandra stores data replicas on multiple nodes to ensure reliability and fault tolerance. The replication strategy for each Edge keyspace determines the nodes where replicas are placed.&nbsp;</p>
<p>The total number of replicas for a keyspace across a Cassandra cluster is referred to as the keyspace's <em>replication factor</em>. A replication factor of one means that there is only one copy of each row in the Cassandra cluster. &nbsp;A replication factor of two means there are two copies of each row, where each copy is on a different node. All replicas are equally important; there is no primary or master replica.&nbsp;</p>
<p>In a production system with three or more Cassandra nodes in each data center, the default replication factor for an Edge keyspace is three. As a general rule, the replication factor should not exceed the number of Cassandra nodes in the cluster.&nbsp;</p>
<p>Use the following procedure to view the Cassandra schema, which shows the replication factor for each Edge keyspace:</p>
<ol>
	<li>Log in to a Cassandra node.</li>
	<li>Run the following command:<br />
		<span style="font-family:courier new,courier,monospace;">&gt; /opt/apigee/apigee-cassandra/bin/cassandra-cli -h $(hostname -i) &lt;&lt;&lt; "show schema;"</span><br />
		<br />
		Where <span style="font-family:courier new,courier,monospace;">$(hostname -i)</span> resolves to the IP address of the Cassandra node. Or you can replace <span style="font-family:courier new,courier,monospace;">$(hostname -i)</span> with the IP address of the node.</li>
</ol>
<p>For each keyspace, you will see output in the form:&nbsp;</p>
<pre>
create keyspace kms
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {dc-1 : 3}
  and durable_writes = true;</pre>
<p>You can see that for data center 1, <span style="font-family:courier new,courier,monospace;">dc-1</span>, the default replication factor for the <span style="font-family:courier new,courier,monospace;">kms</span> keyspace is three for an installation with three Cassandra nodes.</p>
<div class="note">
	<p>For a smaller Cassandra cluster, such as a single node Cassandra cluster used in an Edge all-in-one installation, the replication factor is one. However, an Edge production cluster should always have three or more Cassandra nodes.</p>
</div>
<p>If you add additional Cassandra nodes to the cluster, the default replication factor is not affected. However, if you decide that you want to increase the replication factor, contact Apigee.&nbsp;</p>
<p>For example, if you increase the number of Cassandra nodes to six, but leave the replication factor at three, you do not ensure that all Cassandra nodes have a copy of all the data. If a node goes down, a higher replication factor means a higher probability that the data on the node exists on one of the remaining nodes. The downside of a higher replication factor is an increased latency on data writes. &nbsp;</p>
<h2>About the Cassandra consistency level</h2>
<p>The Cassandra consistency level is defined as the minimum number of Cassandra nodes that must acknowledge a read or write operation before the operation can be considered successful. Different consistency levels can be assigned to different Edge keyspaces.</p>
<p>When connecting to Cassandra for read and write operations, Message Processor and Management Server nodes typically use the Cassandra value of <em><strong>LOCAL_QUORUM</strong></em> to specify the consistency level for a keyspace. However, some keyspaces are defined to use a consistency level of one. &nbsp;</p>
<p>The calculation of the value of <strong>LOCAL_QUORUM</strong> for a data center is:</p>
<pre>
LOCAL_QUORUM = (replication_factor/2) + 1 </pre>
<p>As described above, the default replication factor for an Edge production environment with three Cassandra nodes is three. Therefore, the default value of <strong>LOCAL_QUORUM</strong> = (3/2) +1 = 2 (the value is rounded down to an integer).</p>
<p>With <strong>LOCAL_QUORUM</strong> = 2, at least two of the three Cassandra nodes in the data center must respond to a read/write operation for the operation to succeed. For a three node Cassandra cluster, the cluster could therefore tolerate one node being down per data center. &nbsp;</p>
<p>By specifying the consistency level as <strong>LOCAL_QUORUM</strong>, Edge avoids the latency required by validating operations across multiple data centers. If a keyspace used the Cassandra <strong>QUORUM</strong> value as the consistency level, read/write operations would have to be validated across all data centers.&nbsp;</p>
<p>To see the consistency level used by the Edge Message Processor or Management Server nodes:</p>
<ol>
	<li>Log in to a Message Processor node.</li>
	<li>Change to the /opt/apigee/edge-message-processor/conf directory:<br />
		<span style="font-family:courier new,courier,monospace;">&gt;&nbsp;cd /opt/apigee/edge-message-processor/conf</span></li>
	<li>For write consistency:<br />
		<span style="font-family:courier new,courier,monospace;">&gt;&nbsp;grep -ri "write.consistencylevel" *</span></li>
	<li>For read consistency:<br />
		<span style="font-family:courier new,courier,monospace;">&gt;&nbsp;grep -ri "read.consistencylevel" *</span></li>
	<li>Log in to the Management Server node.</li>
	<li>Change to the /opt/apigee/edge-management-server/conf directory:<br />
		<span style="font-family:courier new,courier,monospace;">&gt;&nbsp;cd /opt/apigee/edge-management-server/conf</span></li>
	<li>Repeat steps 3 and 4.</li>
</ol>
<p>If you add additional Cassandra nodes to the cluster, the consistency level is not affected. However, if you decide that you want to change the consistency level, contact Apigee.&nbsp;</p>
